---
title: "A vector quantized masked autoencoder for speech emotion recognition"
collection: publications
permalink: /publication/2023-04-14-paper-VQMAES-number-2
excerpt: 'Combined VQ-VAE (unsupervised) with MAE (self-supervised) for speech emotion recognition.'
date: 2023-04-14
venue: 'Workshop ICASSP (SASB)'
paperurl: <a href='https://arxiv.org/pdf/2304.11117.pdf'> &#9741;</a>
citation: 'Sadok Samir, Simon Leglaive and Renaud Séguier. “A vector quantized masked autoencoder for speech emotion recognition.” (2023).'
---

<p style="text-align: center;"><a href="https://samsad35.github.io/VQ-MAE-Speech/">Demo</a> - <a href="https://arxiv.org/pdf/2304.11117.pdf">Paper</a></p> 

Recent years have seen remarkable progress in speech emotion recognition (SER), thanks to advances in deep learning techniques. However, the limited availability of labeled data remains a significant challenge in the field. Self-supervised learning has recently emerged as a promising solution to address this challenge. In this paper, we propose the vector quantized masked autoencoder for speech (VQ-MAE-S), a self-supervised model that is fine-tuned to recognize emotions from speech signals. The VQ-MAE-S model is based on a masked autoencoder (MAE) that operates in the discrete latent space of a vector-quantized variational autoencoder. Experimental results show that the proposed VQ-MAE-S model, pre-trained on the VoxCeleb2 dataset and fine-tuned on emotional speech data, outperforms an MAE working on the raw spectrogram representation and other state-of-the-art methods in SER.
